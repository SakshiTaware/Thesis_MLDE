{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee2e6289",
   "metadata": {},
   "source": [
    "## Model Training, Evaluation and Optimization\n",
    "\n",
    "In this section we load the input data, divide it into training set and testing set, set up cross-validation and tune hyperparameters, train the model, optimize it and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14c523",
   "metadata": {},
   "source": [
    "<center> Hyperparameter optimization details <center><br>\n",
    "\n",
    "| Model | Hyperparameters | Fixed parameters |\n",
    "|:-----:|:---------------:|:----------------:|\n",
    "| PLS   | `n_components` = 1, 2, 4, 10 | – |\n",
    "| RF    | `n_tree` = 500, 1000, 1500<br>`max_depth` = None, 5, 10 | – |\n",
    "| SVR   | `C` = 1, 10, 100<br>`epsilon` = 0.1, 0.2, 0.5 | `kernel` = rbf |\n",
    "| CNN   | `batch size` = 15, 20, 32<br>`layers` = [32, 64, 128]<br>`number of dense layers` = 1 or 2 | `optimizer` = adam<br>`objective` = mse, mae<br>`training epochs` = 500<br>`patience` = 20 |\n",
    "| MLP   | `batch size` = 15, 20, 32<br>`layers` = [256, 128, 64]<br>`number of dense layers` = 1 | `optimizer` = adam<br>`objective` = mse, mae<br>`training epochs` = 500<br>`patience` = 5 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f97bfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import dependecies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# scikit learn dependencies\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# models\n",
    "import sklearn\n",
    "#import scikeras\n",
    "from sklearn.cross_decomposition import PLSRegression # partial least squares regression\n",
    "from sklearn.ensemble import RandomForestRegressor # random forest\n",
    "from sklearn.svm import SVR # support vector regression\n",
    "\n",
    "# to build neural networks (cnn, mlp)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "# To estimate model performance\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error \n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# For cross-validation \n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b10c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoded dataset \n",
    "protein_sequences_file = 'aaindex_encoded.csv'  # CSV file path\n",
    "df = pd.read_csv(protein_sequences_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8c3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# If columns (other that encoded data) need to be removed\n",
    "df = df.drop([\"sequence\", \"Fitness\"], axis=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7e8659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare X variables i.e. encoded data\n",
    "\n",
    "X = df.to_numpy() #convert to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e070ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# for AAindex, if encoded properties have not been combined\n",
    "\n",
    "# First, convert the string lists to real lists\n",
    "for col in [\"CIDH920105\", \"KYTJ820101\", \"CHOP780201\", \"GRAR740102\", \n",
    "            \"HOPT810101\", \"ZIMJ680104\",\"KARS160118\",\"BUNA790103\"]:\n",
    "    df[col] = df[col].apply(ast.literal_eval)\n",
    "\n",
    "# Now each cell in these columns is a real Python list, not a string!\n",
    "\n",
    "# Next, flatten the lists row-wise\n",
    "df['encoded_combined'] = df.apply(lambda row: np.concatenate([row[col] for col in [\n",
    "    \"CIDH920105\", \"KYTJ820101\", \"CHOP780201\", \"GRAR740102\", \n",
    "    \"HOPT810101\", \"ZIMJ680104\",\"KARS160118\",\"BUNA790103\"\n",
    "]]), axis=1)\n",
    "\n",
    "# Finally, convert to a numeric matrix\n",
    "X = np.stack(df['encoded_combined'].values)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ca3265",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# For Z-scales or other encodings, if the encoded data is not flattened \n",
    "\n",
    "# Convert the string representation of lists into actual lists\n",
    "df[\"encoded_sequence\"] = df[\"encoded_sequence\"].apply(ast.literal_eval)\n",
    "\n",
    "# Convert each sequence into a flattened numerical array\n",
    "df[\"flattened_sequence\"] = df[\"encoded_sequence\"].apply(lambda x: np.array(x).flatten())\n",
    "\n",
    "# Expand into a proper feature matrix\n",
    "X = np.vstack(df[\"flattened_sequence\"])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff4a5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset with fitness values (in case it is in a different file or the column has been removed)\n",
    "df2 = pd.read_csv(\"ExampleData.csv\")\n",
    "\n",
    "# prepare Y variable i.e fitness values\n",
    "\n",
    "Y = df2[\"fitness\"].to_numpy(dtype=np.float32) # fitness variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d269ac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check shape of X and Y (num_samples, num_features) \n",
    "\n",
    "print(\"X Shape:\", X.shape)  \n",
    "print(\"Y Shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b0c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (adjust test_size, random_state constant)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6424ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation using K-fold \n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "def cross_validation(model, X, Y, cv):\n",
    "    cv_score_r2 = cross_val_score(model, X, Y, scoring='r2', cv = cv)\n",
    "    cv_score_mse = cross_val_score(model, X, Y, scoring='neg_mean_squared_error', cv = cv)\n",
    "    cv_score_rmse = np.sqrt(-cv_score_mse)\n",
    "\n",
    "    print(f\"Cross-Validation R² scores: {cv_score_r2:.3f}\")\n",
    "    print(f\"Mean R²: {cv_score_r2.mean():.3f}\")\n",
    "    print(f\"Std of R²: {cv_score_r2.std():.5f}\")\n",
    "    print(\" \")\n",
    "    print(f\"Cross-Validation RMSE scores: {cv_score_rmse:.5f}\")\n",
    "    print(f\"Mean RMSE: {cv_score_rmse.mean():.5f}\")\n",
    "    print(f\"Std of RMSE: {cv_score_rmse.std():.5f}\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4371b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance function\n",
    "\n",
    "def model_performance(Y_train, Y_test, Y_train_pred, Y_test_pred):\n",
    "    r2_train = r2_score(Y_train, Y_train_pred)\n",
    "    r2_test = r2_score(Y_test, Y_test_pred)\n",
    "    mse_train = mean_squared_error(Y_train, Y_train_pred)\n",
    "    mse_test = mean_squared_error(Y_test, Y_test_pred)\n",
    "    mae_train = mean_absolute_error(Y_train, Y_train_pred)\n",
    "    mae_test = mean_absolute_error(Y_test, Y_test_pred)\n",
    "    rmse_train = np.sqrt(mean_squared_error(Y_train, Y_train_pred))\n",
    "    rmse_test = np.sqrt(mean_squared_error(Y_test, Y_test_pred))\n",
    "\n",
    "    print(f\"Model Performance:\")\n",
    "    print(f\"R² (Train): {r2_train:.3f}, R² (Test): {r2_test:.3f}\")\n",
    "    print(f\"MAE (Train): {mae_train:.5f}, MAE (Test): {mae_test:.5f}\")\n",
    "    print(f\"MSE (Train): {mse_train:.5f}, MSE (Test): {mse_test:.5f}\")\n",
    "    print(f\"RMSE (Train): {rmse_train:.5f}, RMSE (Test): {rmse_test:.5f}\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e97beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a21c9d63",
   "metadata": {},
   "source": [
    "### Partial Least Squares (PLS) Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67402a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' #In case standardization is needed\n",
    "\n",
    "# Standardize Features for PLS \n",
    "scaler_X = StandardScaler()\n",
    "scaler_Y = StandardScaler()\n",
    "\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "Y_train = scaler_Y.fit_transform(Y_train)\n",
    "Y_test = scaler_Y.transform(Y_test)\n",
    "'''\n",
    "\n",
    "# Training PLS Regression \n",
    "\n",
    "pls = PLSRegression(scale = True) \n",
    "pls_params = {'n_components': [1, 2, 4, 10]} # hyperparameters for grid search\n",
    "pls_grid = GridSearchCV(pls, pls_params, scoring ='r2', cv = 5) \n",
    "pls_grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b006d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "pls_best_model = pls_grid.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "Y_train_pred = pls_best_model.predict(X_train)\n",
    "Y_test_pred = pls_best_model.predict(X_test)\n",
    "\n",
    "'''\n",
    "# Reverse Scaling (Optional: Convert Predictions Back to Original Scale)\n",
    "Y_train_pred = scaler_Y.inverse_transform(Y_train_pred)\n",
    "Y_test_pred = scaler_Y.inverse_transform(Y_test_pred)\n",
    "'''\n",
    "\n",
    "# Model Evaluation\n",
    "model_performance(Y_train, Y_test, Y_train_pred, Y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f5b6e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#To save GridSearchCV results \n",
    "results = pd.DataFrame(pls_grid.cv_results_)\n",
    "results.to_csv('results_aaindex_pls.csv', index=False) # save as csv\n",
    "\n",
    "# display(results[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]) # to display main results\n",
    "# results # to display all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb290e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d4d3f85",
   "metadata": {},
   "source": [
    "### Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc1897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training RF\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf_params = {'max_depth': [None, 5, 10], \"n_estimators\": [500, 1000, 1500]} # hyperparameters for grid search\n",
    "rf_grid = GridSearchCV(rf, rf_params, cv = 5, scoring='r2', verbose = 1, error_score='raise')\n",
    "rf_grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b9d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "rf_best_model = rf_grid.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "Y_train_pred = rf_best_model.predict(X_train)\n",
    "Y_test_pred = rf_best_model.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "model_performance(Y_train, Y_test, Y_train_pred, Y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1bc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save GridSearchCV results \n",
    "results = pd.DataFrame(rf_grid.cv_results_)\n",
    "results.to_csv('results_aaindex_rf.csv', index=False)\n",
    "\n",
    "# display(results[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]) # to display main results\n",
    "# results # to display all results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814dd2c2",
   "metadata": {},
   "source": [
    "### Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' #In case standardization is needed\n",
    "\n",
    "# Standardize the features \n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "# Optional: standardize target if it has large variance\n",
    "scaler_Y = StandardScaler()\n",
    "Y_scaled = scaler_Y.fit_transform(Y.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Split data (adjust test_size, keep random_state the same)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_scaled, test_size=0.3, random_state=42)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4605d57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training SVR model\n",
    "svr = SVR()\n",
    "svr_params = {'C':[1, 10, 100], 'epsilon':[0.1, 0.2, 0.5], 'kernel':['rbf']} # hyperparameters for grid search\n",
    "svr_grid = GridSearchCV(svr, svr_params, scoring='r2', cv=5, verbose=1)\n",
    "svr_grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72ee46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "svr_best_model = svr_grid.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "Y_train_pred = svr_best_model.predict(X_train)\n",
    "Y_test_pred = svr_best_model.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "model_performance(Y_train, Y_test, Y_train_pred, Y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357435e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save GridSearchCV results\n",
    "\n",
    "results = pd.DataFrame(svr_grid.cv_results_)\n",
    "results.to_csv('results_aaindex_svr_.csv', index=False)\n",
    "\n",
    "# display(results[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]) # to display main results\n",
    "# results # to display all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf2a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "673bc633",
   "metadata": {},
   "source": [
    "### Convolution Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input for CNN\n",
    "\n",
    "'''\n",
    "# if sequence length of encoded sequence is unknown\n",
    "total_features = X.shape[1]        # total features\n",
    "feat_dim = 5               # for example z-scales feature dimension is 5\n",
    "\n",
    "seq_len = total_features // feat_dim\n",
    "print(\"Sequence length:\", seq_len)\n",
    "'''\n",
    "\n",
    "seq_len  = 145   \n",
    "feat_dim = 5 # for example z-scales feature dimension is 5\n",
    "\n",
    "X_train_cnn = X_train.reshape(-1, seq_len, feat_dim)  \n",
    "X_test_cnn  = X_test .reshape(-1, seq_len, feat_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2225966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN architecture\n",
    "\n",
    "cnn = models.Sequential([\n",
    "    layers.Input(shape=(seq_len, feat_dim)),      \n",
    "    layers.Conv1D(32, kernel_size=5, activation=\"relu\", padding=\"same\"),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Conv1D(64, kernel_size=3, activation=\"relu\", padding=\"same\"),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "cnn.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mse\", \"mae\"])\n",
    "\n",
    "early = callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n",
    "\n",
    "history_cnn = cnn.fit(\n",
    "    X_train_cnn, Y_train,\n",
    "    epochs=300,\n",
    "    batch_size=15,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early],\n",
    "    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ea4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "Y_train_pred = cnn.predict(X_train_cnn)\n",
    "Y_test_pred = cnn.predict(X_test_cnn)\n",
    "\n",
    "# Cross-validation and Model Evaluation\n",
    "\n",
    "cross_validation(cnn, X, Y, kf)\n",
    "model_performance(Y_train, Y_test, Y_train_pred, Y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133c73a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b462a8a0",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bc46b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build MLP architecture\n",
    "\n",
    "# Optionally scale inputs\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_mlp = scaler.transform(X_train)\n",
    "X_test_mlp  = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "mlp = models.Sequential([\n",
    "    layers.Input(shape=(X_train_mlp.shape[1],)),\n",
    "    layers.Dense(256, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation=\"sigmoid\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation=\"softmax\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1)                     # regression output\n",
    "])\n",
    "\n",
    "\n",
    "mlp.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mse\", \"mae\"])\n",
    "\n",
    "early = callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "history_mlp = mlp.fit(\n",
    "    X_train_mlp, Y_train,\n",
    "    epochs=300,\n",
    "    batch_size=15,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early],\n",
    "    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "Y_train_pred = mlp.predict(X_train_mlp)\n",
    "Y_test_pred = mlp.predict(X_test_mlp)\n",
    "\n",
    "# Cross-validation and Model Evaluation\n",
    "\n",
    "cross_validation(mlp, X, Y, kf)\n",
    "model_performance(Y_train, Y_test, Y_train_pred, Y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aff898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# MLP using GridSearch # facing issues\n",
    "\n",
    "start=time()\n",
    "\n",
    "# define a function to create model, required for KerasClassifier\n",
    "# the function takes drop_out rate as argument so we can optimize it  \n",
    "def create_mlp_model(dropout_rate=0.3):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu', input_shape=(X_train_mlp.shape[1],))) \n",
    "    # add a dropout layer if rate is not null\n",
    "    if dropout_rate != 0:\n",
    "        model.add(Dropout(rate=dropout_rate))        \n",
    "    model.add(Dense(128, activation='relu')) \n",
    "    # add a dropout layer if rate is not null    \n",
    "    if dropout_rate != 0:\n",
    "        model.add(Dropout(rate=dropout_rate))           \n",
    "    model.add(Dense(64, activation='sigmoid')) \n",
    "    # add a dropout layer if rate is not null    \n",
    "    if dropout_rate != 0:\n",
    "        model.add(Dropout(rate=dropout_rate))           \n",
    "    model.add(Dense(1, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile( \n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\"mse\", \"mae\", \"accuracy\"],\n",
    "        )    \n",
    "    return model\n",
    "    \n",
    "# define function to display the results of the grid search\n",
    "def display_cv_results(search_results):\n",
    "    print('Best score = {:.4f} using {}'.format(search_results.best_score_, search_results.best_params_))\n",
    "    means = search_results.cv_results_['mean_test_score']\n",
    "    stds = search_results.cv_results_['std_test_score']\n",
    "    params = search_results.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print('mean test accuracy +/- std = {:.4f} +/- {:.4f} with: {}'.format(mean, stdev, param))    \n",
    "    \n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_mlp_model, verbose=1)\n",
    "# define parameters and values for grid search \n",
    "param_grid = {\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [n_epochs_cv],\n",
    "    'dropout_rate': [0.0, 0.10, 0.20, 0.30],\n",
    "}\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)\n",
    "grid_result = grid.fit(X, to_categorical(Y))  # fit the full dataset as we are using cross validation \n",
    "\n",
    "\n",
    "display_cv_results(grid_result) # display full cv results\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b9ea8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# to check loss during neural network training\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_mlp.history['loss'], label='Training Loss')\n",
    "plt.plot(history_mlp.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c5b074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10env",
   "language": "python",
   "name": "py3.10env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
